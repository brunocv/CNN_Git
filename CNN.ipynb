{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n",
    "\n",
    "### Part 1 - Building the CNN\n",
    "\n",
    "#### Importing the Tensorflow libraries and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = 'dataset/train'\n",
    "TEST_DIR = 'dataset/test'\n",
    "VALID_DIR = 'dataset/valid'\n",
    "IMG_SIZE = 255 \n",
    "LR = 1e-3 #learn rate, quanto menor mais aprende"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'CNN-{}-{}.model'.format(LR, 'Xconv-basic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()\n",
    "\n",
    "#classes = 250\n",
    "\n",
    "#Kernels usually 1x1 | 3x3 | 5x5\n",
    "\n",
    "#Convolution + MaxPooling layer\n",
    "# Arguments: 32 filters, 3x3 kernel, relu activation\n",
    "classifier.add(Conv2D(32, (3, 3), input_shape = (IMG_SIZE, IMG_SIZE, 3), padding='same', activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "#Adding a second convolutional layer\n",
    "classifier.add(Conv2D(64, (3, 3), padding='same', activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "#Adding a third convolutional layer\n",
    "classifier.add(Conv2D(128, (3, 3), padding='same', activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "#Adding a fourth convolutional layer\n",
    "classifier.add(Conv2D(256, (3, 3), padding='same', activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "classifier.add(Flatten())\n",
    "\n",
    "classifier.add(Dense(units = 1024, activation = 'relu'))\n",
    "#classifier.add(Dropout(0.5))\n",
    "classifier.add(Dense(units = 250, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's double check the model description:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 255, 255, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 127, 127, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 127, 127, 64)      18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 63, 63, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 63, 63, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 31, 31, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 31, 31, 256)       295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 15, 15, 256)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 57600)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              58983424  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 250)               256250    \n",
      "=================================================================\n",
      "Total params: 59,628,090\n",
      "Trainable params: 59,628,090\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 - Fitting the CNN to the images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./IMG_SIZE,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./IMG_SIZE)\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale = 1./IMG_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 35215 images belonging to 250 classes.\n",
      "Found 1250 images belonging to 250 classes.\n",
      "Found 1250 images belonging to 250 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set = train_datagen.flow_from_directory(TRAIN_DIR,\n",
    "                                                 target_size = (IMG_SIZE, IMG_SIZE),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'categorical')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(TEST_DIR,\n",
    "                                            target_size = (IMG_SIZE, IMG_SIZE),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'categorical')\n",
    "\n",
    "validation_set = validation_datagen.flow_from_directory(VALID_DIR,\n",
    "                                            target_size = (IMG_SIZE, IMG_SIZE),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from datetime import datetime\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', patience=5)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', save_best_only=True)\n",
    "\n",
    "# Create a TensorBoard callback\n",
    "logs = \"logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard = TensorBoard(log_dir = logs,\n",
    "                          histogram_freq = 1,\n",
    "                          profile_batch = '500,520')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1101/1101 [==============================] - 1672s 2s/step - loss: 0.0360 - accuracy: 0.0203 - val_loss: 0.0175 - val_accuracy: 0.2528\n",
      "Epoch 2/25\n",
      "1101/1101 [==============================] - 1703s 2s/step - loss: 0.0168 - accuracy: 0.2965 - val_loss: 0.0108 - val_accuracy: 0.5520\n",
      "Epoch 3/25\n",
      "1101/1101 [==============================] - 1703s 2s/step - loss: 0.0117 - accuracy: 0.5233 - val_loss: 0.0084 - val_accuracy: 0.6768\n",
      "Epoch 4/25\n",
      "1101/1101 [==============================] - 1656s 2s/step - loss: 0.0091 - accuracy: 0.6399 - val_loss: 0.0077 - val_accuracy: 0.7048\n",
      "Epoch 5/25\n",
      "1101/1101 [==============================] - 1665s 2s/step - loss: 0.0072 - accuracy: 0.7342 - val_loss: 0.0066 - val_accuracy: 0.7624\n",
      "Epoch 6/25\n",
      "1101/1101 [==============================] - 1674s 2s/step - loss: 0.0058 - accuracy: 0.7920 - val_loss: 0.0064 - val_accuracy: 0.7688\n",
      "Epoch 7/25\n",
      "1101/1101 [==============================] - 1645s 1s/step - loss: 0.0048 - accuracy: 0.8382 - val_loss: 0.0064 - val_accuracy: 0.7720\n",
      "Epoch 8/25\n",
      "1101/1101 [==============================] - 1689s 2s/step - loss: 0.0041 - accuracy: 0.8689 - val_loss: 0.0065 - val_accuracy: 0.7848\n",
      "Epoch 9/25\n",
      "1101/1101 [==============================] - 1684s 2s/step - loss: 0.0035 - accuracy: 0.8960 - val_loss: 0.0064 - val_accuracy: 0.7968\n",
      "Epoch 10/25\n",
      "1101/1101 [==============================] - 1713s 2s/step - loss: 0.0029 - accuracy: 0.9187 - val_loss: 0.0071 - val_accuracy: 0.7824\n",
      "Epoch 11/25\n",
      "1101/1101 [==============================] - 1699s 2s/step - loss: 0.0027 - accuracy: 0.9290 - val_loss: 0.0070 - val_accuracy: 0.7928\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x297a3f75640>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit( training_set,\n",
    "                         epochs = 25,\n",
    "                         callbacks = [es, mc, tensorboard],\n",
    "                         validation_data = validation_set,\n",
    "                         workers = 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
